{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db965314",
   "metadata": {},
   "source": [
    "This code example solves the CartPole-v0 environment using a Proximal Policy Optimization (PPO) agent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9525c7b1",
   "metadata": {},
   "source": [
    "#### Task description :\n",
    "The system consists of a cart moving on a track with a pole attached to it via an un-powered joint. The cart can be pushed with a force of either +1 or -1, and the objective is to keep the pole upright to receive a reward of +1 for each time step that it remains so. The episode terminates if the pole tilts more than 15 degrees from vertical or if the cart moves more than 2.4 units from the center. The episode lasts for a maximum of 200 time steps, resulting in a maximum return of 200.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39df71b5",
   "metadata": {},
   "source": [
    "#### Proximal Policy Optimization algorithm implementation :"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "67f51c06",
   "metadata": {},
   "source": [
    "<img src=\"PPO-Clip algo.png\" width=50%>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63241661",
   "metadata": {},
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7207d6e1",
   "metadata": {},
   "source": [
    "**For this example the following libraries are used:**\n",
    "\n",
    "- numpy for n-dimensional arrays\n",
    "- tensorflow and keras for building the deep RL PPO agent\n",
    "- gym for getting everything we need about the environment\n",
    "- scipy.signal for calculating the discounted cumulative sums of vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "53bc6af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import gym\n",
    "import scipy.signal\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07772915",
   "metadata": {},
   "source": [
    "### Functions and class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba757eca",
   "metadata": {},
   "source": [
    "The code consists of several functions for building and training the neural networks used in the algorithm. The __discounted_cumulative_sums__ function computes the discounted cumulativ sum of rewards and advantages used for updating the policy and value functions. The __Buffer__ class stores the trajectories of the agent's interactions with the environment, and computes the advantages and rewards-to-go of each trajectory. The mlp function builds a feedforward neural network with given sizes, and the __logprobabilities__ function computes the log-probabilities of taking actions based on the actor's output logits.\n",
    "\n",
    "The code also includes several training functions. __sample_action__ is a function that samples actions from the actor's output logits. The __train_policy__ function trains the policy by maximizing the PPO-Clip objective, and __train_value_function__ trains the value function by regression on mean-squared error. Both training functions use automatic differentiation to compute the gradients of the loss functions with respect to the model weights, and apply the gradients using the specified optimizers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "46bc5bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def discounted_cumulative_sums(x, discount):\n",
    "    # Discounted cumulative sums of vectors for computing rewards-to-go and advantage estimates\n",
    "    return scipy.signal.lfilter([1], [1, float(-discount)], x[::-1], axis=0)[::-1]\n",
    "\n",
    "\n",
    "class Buffer:\n",
    "    # Buffer for storing trajectories\n",
    "    def __init__(self, observation_dimensions, size, gamma=0.99, lam=0.95):\n",
    "        # Buffer initialization\n",
    "        self.observation_buffer = np.zeros(\n",
    "            (size, observation_dimensions), dtype=np.float32\n",
    "        )\n",
    "        self.action_buffer = np.zeros(size, dtype=np.int32)\n",
    "        self.advantage_buffer = np.zeros(size, dtype=np.float32)\n",
    "        self.reward_buffer = np.zeros(size, dtype=np.float32)\n",
    "        self.return_buffer = np.zeros(size, dtype=np.float32)\n",
    "        self.value_buffer = np.zeros(size, dtype=np.float32)\n",
    "        self.logprobability_buffer = np.zeros(size, dtype=np.float32)\n",
    "        self.gamma, self.lam = gamma, lam\n",
    "        self.pointer, self.trajectory_start_index = 0, 0\n",
    "\n",
    "    def store(self, observation, action, reward, value, logprobability):\n",
    "        # Append one step of agent-environment interaction\n",
    "        self.observation_buffer[self.pointer] = observation\n",
    "        self.action_buffer[self.pointer] = action\n",
    "        self.reward_buffer[self.pointer] = reward\n",
    "        self.value_buffer[self.pointer] = value\n",
    "        self.logprobability_buffer[self.pointer] = logprobability\n",
    "        self.pointer += 1\n",
    "\n",
    "    def finish_trajectory(self, last_value=0):\n",
    "        # Finish the trajectory by computing advantage estimates and rewards-to-go\n",
    "        path_slice = slice(self.trajectory_start_index, self.pointer)\n",
    "        rewards = np.append(self.reward_buffer[path_slice], last_value)\n",
    "        values = np.append(self.value_buffer[path_slice], last_value)\n",
    "\n",
    "        deltas = rewards[:-1] + self.gamma * values[1:] - values[:-1]\n",
    "\n",
    "        self.advantage_buffer[path_slice] = discounted_cumulative_sums(\n",
    "            deltas, self.gamma * self.lam\n",
    "        )\n",
    "        self.return_buffer[path_slice] = discounted_cumulative_sums(\n",
    "            rewards, self.gamma\n",
    "        )[:-1]\n",
    "\n",
    "        self.trajectory_start_index = self.pointer\n",
    "\n",
    "    def get(self):\n",
    "        # Get all data of the buffer and normalize the advantages\n",
    "        self.pointer, self.trajectory_start_index = 0, 0\n",
    "        advantage_mean, advantage_std = (\n",
    "            np.mean(self.advantage_buffer),\n",
    "            np.std(self.advantage_buffer),\n",
    "        )\n",
    "        self.advantage_buffer = (self.advantage_buffer - advantage_mean) / advantage_std\n",
    "        return (\n",
    "            self.observation_buffer,\n",
    "            self.action_buffer,\n",
    "            self.advantage_buffer,\n",
    "            self.return_buffer,\n",
    "            self.logprobability_buffer,\n",
    "        )\n",
    "\n",
    "\n",
    "def mlp(x, sizes, activation=tf.tanh, output_activation=None):\n",
    "    # Build a feedforward neural network\n",
    "    for size in sizes[:-1]:\n",
    "        x = layers.Dense(units=size, activation=activation)(x)\n",
    "    return layers.Dense(units=sizes[-1], activation=output_activation)(x)\n",
    "\n",
    "\n",
    "def logprobabilities(logits, a):\n",
    "    # Compute the log-probabilities of taking actions a by using the logits (i.e. the output of the actor)\n",
    "    logprobabilities_all = tf.nn.log_softmax(logits)\n",
    "    logprobability = tf.reduce_sum(\n",
    "        tf.one_hot(a, num_actions) * logprobabilities_all, axis=1\n",
    "    )\n",
    "    return logprobability\n",
    "\n",
    "\n",
    "# Sample action from actor\n",
    "@tf.function\n",
    "def sample_action(observation):\n",
    "    logits = actor(observation)\n",
    "    action = tf.squeeze(tf.random.categorical(logits, 1), axis=1)\n",
    "    return logits, action\n",
    "\n",
    "\n",
    "# Train the policy by maxizing the PPO-Clip objective\n",
    "@tf.function\n",
    "def train_policy(\n",
    "    observation_buffer, action_buffer, logprobability_buffer, advantage_buffer\n",
    "):\n",
    "\n",
    "    with tf.GradientTape() as tape:  # Record operations for automatic differentiation.\n",
    "        ratio = tf.exp(\n",
    "            logprobabilities(actor(observation_buffer), action_buffer)\n",
    "            - logprobability_buffer\n",
    "        )\n",
    "        min_advantage = tf.where(\n",
    "            advantage_buffer > 0,\n",
    "            (1 + clip_ratio) * advantage_buffer,\n",
    "            (1 - clip_ratio) * advantage_buffer,\n",
    "        )\n",
    "\n",
    "        policy_loss = -tf.reduce_mean(\n",
    "            tf.minimum(ratio * advantage_buffer, min_advantage)\n",
    "        )\n",
    "    policy_grads = tape.gradient(policy_loss, actor.trainable_variables)\n",
    "    policy_optimizer.apply_gradients(zip(policy_grads, actor.trainable_variables))\n",
    "\n",
    "    kl = tf.reduce_mean(\n",
    "        logprobability_buffer\n",
    "        - logprobabilities(actor(observation_buffer), action_buffer)\n",
    "    )\n",
    "    kl = tf.reduce_sum(kl)\n",
    "    return kl\n",
    "\n",
    "\n",
    "# Train the value function by regression on mean-squared error\n",
    "@tf.function\n",
    "def train_value_function(observation_buffer, return_buffer):\n",
    "    with tf.GradientTape() as tape:  # Record operations for automatic differentiation.\n",
    "        value_loss = tf.reduce_mean((return_buffer - critic(observation_buffer)) ** 2)\n",
    "    value_grads = tape.gradient(value_loss, critic.trainable_variables)\n",
    "    value_optimizer.apply_gradients(zip(value_grads, critic.trainable_variables))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34e4fb1c",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ee1b469f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters of the PPO algorithm\n",
    "steps_per_epoch = 4000\n",
    "epochs = 30\n",
    "gamma = 0.99\n",
    "clip_ratio = 0.2\n",
    "policy_learning_rate = 3e-4\n",
    "value_function_learning_rate = 1e-3\n",
    "train_policy_iterations = 80\n",
    "train_value_iterations = 80\n",
    "lam = 0.97\n",
    "target_kl = 0.01\n",
    "hidden_sizes = (64, 64)\n",
    "\n",
    "# True if you want to render the environment\n",
    "render = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03ff8cb6",
   "metadata": {},
   "source": [
    "### Initializations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f4ae1c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the environment and get the dimensionality of the\n",
    "# observation space and the number of possible actions\n",
    "env = gym.make(\"CartPole-v0\")\n",
    "observation_dimensions = env.observation_space.shape[0]\n",
    "num_actions = env.action_space.n\n",
    "\n",
    "# Initialize the buffer\n",
    "buffer = Buffer(observation_dimensions, steps_per_epoch)\n",
    "\n",
    "# Initialize the actor and the critic as keras models\n",
    "observation_input = keras.Input(shape=(observation_dimensions,), dtype=tf.float32)\n",
    "logits = mlp(observation_input, list(hidden_sizes) + [num_actions], tf.tanh, None)\n",
    "actor = keras.Model(inputs=observation_input, outputs=logits)\n",
    "value = tf.squeeze(\n",
    "    mlp(observation_input, list(hidden_sizes) + [1], tf.tanh, None), axis=1\n",
    ")\n",
    "critic = keras.Model(inputs=observation_input, outputs=value)\n",
    "\n",
    "# Initialize the policy and the value function optimizers\n",
    "policy_optimizer = keras.optimizers.Adam(learning_rate=policy_learning_rate)\n",
    "value_optimizer = keras.optimizers.Adam(learning_rate=value_function_learning_rate)\n",
    "\n",
    "# Initialize the observation, episode return and episode length\n",
    "observation, episode_return, episode_length = env.reset(), 0, 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "402bcdb1",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "73554139",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch: 1. Mean Return: 26.490066225165563. Mean Length: 26.490066225165563\n",
      " Epoch: 2. Mean Return: 32.78688524590164. Mean Length: 32.78688524590164\n",
      " Epoch: 3. Mean Return: 46.51162790697674. Mean Length: 46.51162790697674\n",
      " Epoch: 4. Mean Return: 74.07407407407408. Mean Length: 74.07407407407408\n",
      " Epoch: 5. Mean Return: 88.88888888888889. Mean Length: 88.88888888888889\n",
      " Epoch: 6. Mean Return: 137.93103448275863. Mean Length: 137.93103448275863\n",
      " Epoch: 7. Mean Return: 173.91304347826087. Mean Length: 173.91304347826087\n",
      " Epoch: 8. Mean Return: 173.91304347826087. Mean Length: 173.91304347826087\n",
      " Epoch: 9. Mean Return: 181.8181818181818. Mean Length: 181.8181818181818\n",
      " Epoch: 10. Mean Return: 190.47619047619048. Mean Length: 190.47619047619048\n",
      " Epoch: 11. Mean Return: 190.47619047619048. Mean Length: 190.47619047619048\n",
      " Epoch: 12. Mean Return: 190.47619047619048. Mean Length: 190.47619047619048\n",
      " Epoch: 13. Mean Return: 190.47619047619048. Mean Length: 190.47619047619048\n",
      " Epoch: 14. Mean Return: 190.47619047619048. Mean Length: 190.47619047619048\n",
      " Epoch: 15. Mean Return: 200.0. Mean Length: 200.0\n",
      " Epoch: 16. Mean Return: 200.0. Mean Length: 200.0\n",
      " Epoch: 17. Mean Return: 200.0. Mean Length: 200.0\n",
      " Epoch: 18. Mean Return: 200.0. Mean Length: 200.0\n",
      " Epoch: 19. Mean Return: 181.8181818181818. Mean Length: 181.8181818181818\n",
      " Epoch: 20. Mean Return: 200.0. Mean Length: 200.0\n",
      " Epoch: 21. Mean Return: 200.0. Mean Length: 200.0\n",
      " Epoch: 22. Mean Return: 200.0. Mean Length: 200.0\n",
      " Epoch: 23. Mean Return: 200.0. Mean Length: 200.0\n",
      " Epoch: 24. Mean Return: 200.0. Mean Length: 200.0\n",
      " Epoch: 25. Mean Return: 190.47619047619048. Mean Length: 190.47619047619048\n",
      " Epoch: 26. Mean Return: 200.0. Mean Length: 200.0\n",
      " Epoch: 27. Mean Return: 200.0. Mean Length: 200.0\n",
      " Epoch: 28. Mean Return: 200.0. Mean Length: 200.0\n",
      " Epoch: 29. Mean Return: 200.0. Mean Length: 200.0\n",
      " Epoch: 30. Mean Return: 200.0. Mean Length: 200.0\n"
     ]
    }
   ],
   "source": [
    "# Iterate over the number of epochs\n",
    "for epoch in range(epochs):\n",
    "    # Initialize the sum of the returns, lengths and number of episodes for each epoch\n",
    "    sum_return = 0\n",
    "    sum_length = 0\n",
    "    num_episodes = 0\n",
    "\n",
    "    # Iterate over the steps of each epoch\n",
    "    for t in range(steps_per_epoch):\n",
    "        if render:\n",
    "            env.render()\n",
    "\n",
    "        # Get the logits, action, and take one step in the environment\n",
    "        observation = observation.reshape(1, -1)\n",
    "        logits, action = sample_action(observation)\n",
    "        observation_new, reward, done, _ = env.step(action[0].numpy())\n",
    "        episode_return += reward\n",
    "        episode_length += 1\n",
    "\n",
    "        # Get the value and log-probability of the action\n",
    "        value_t = critic(observation)\n",
    "        logprobability_t = logprobabilities(logits, action)\n",
    "\n",
    "        # Store obs, act, rew, v_t, logp_pi_t\n",
    "        buffer.store(observation, action, reward, value_t, logprobability_t)\n",
    "\n",
    "        # Update the observation\n",
    "        observation = observation_new\n",
    "\n",
    "        # Finish trajectory if reached to a terminal state\n",
    "        terminal = done\n",
    "        if terminal or (t == steps_per_epoch - 1):\n",
    "            last_value = 0 if done else critic(observation.reshape(1, -1))\n",
    "            buffer.finish_trajectory(last_value)\n",
    "            sum_return += episode_return\n",
    "            sum_length += episode_length\n",
    "            num_episodes += 1\n",
    "            observation, episode_return, episode_length = env.reset(), 0, 0\n",
    "\n",
    "    # Get values from the buffer\n",
    "    (\n",
    "        observation_buffer,\n",
    "        action_buffer,\n",
    "        advantage_buffer,\n",
    "        return_buffer,\n",
    "        logprobability_buffer,\n",
    "    ) = buffer.get()\n",
    "\n",
    "    # Update the policy and implement early stopping using KL divergence\n",
    "    for _ in range(train_policy_iterations):\n",
    "        kl = train_policy(\n",
    "            observation_buffer, action_buffer, logprobability_buffer, advantage_buffer\n",
    "        )\n",
    "        if kl > 1.5 * target_kl:\n",
    "            # Early Stopping\n",
    "            break\n",
    "\n",
    "    # Update the value function\n",
    "    for _ in range(train_value_iterations):\n",
    "        train_value_function(observation_buffer, return_buffer)\n",
    "\n",
    "    # Print mean return and length for each epoch\n",
    "    print(\n",
    "        f\" Epoch: {epoch + 1}. Mean Return: {sum_return / num_episodes}. Mean Length: {sum_length / num_episodes}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc92d6f1",
   "metadata": {},
   "source": [
    "### Visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3629d01",
   "metadata": {},
   "source": [
    "first epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95b66744",
   "metadata": {},
   "source": [
    "<img src=\"first epoch.gif\" width=50%>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cdc899d",
   "metadata": {},
   "source": [
    "8 th epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6f47502",
   "metadata": {},
   "source": [
    "<img src=\"8 epochs.gif\" width=50%>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c39a20b",
   "metadata": {},
   "source": [
    "20 th epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5de88431",
   "metadata": {},
   "source": [
    "<img src=\"20 epochs.gif\" width=50%>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
